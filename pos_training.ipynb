{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (/home/emukans/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f54a82145841f8997f51d4d316bf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...</td>\n",
       "      <td>[21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7]</td>\n",
       "      <td>[11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0]</td>\n",
       "      <td>[0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Nadim, Ladki]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[AL-AIN, ,, United, Arab, Emirates, 1996-12-06]</td>\n",
       "      <td>[22, 6, 22, 22, 23, 11]</td>\n",
       "      <td>[11, 0, 11, 12, 12, 12]</td>\n",
       "      <td>[5, 0, 5, 6, 6, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Japan, began, the, defence, of, their, Asian,...</td>\n",
       "      <td>[22, 38, 12, 21, 15, 29, 16, 22, 21, 15, 12, 1...</td>\n",
       "      <td>[11, 21, 11, 12, 13, 11, 12, 12, 12, 13, 11, 1...</td>\n",
       "      <td>[5, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[But, China, saw, their, luck, desert, them, i...</td>\n",
       "      <td>[10, 22, 38, 29, 21, 37, 28, 15, 12, 21, 21, 1...</td>\n",
       "      <td>[0, 11, 21, 11, 12, 21, 11, 13, 11, 12, 12, 13...</td>\n",
       "      <td>[0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                             tokens  \\\n",
       "0  0  [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...   \n",
       "1  1                                     [Nadim, Ladki]   \n",
       "2  2    [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
       "3  3  [Japan, began, the, defence, of, their, Asian,...   \n",
       "4  4  [But, China, saw, their, luck, desert, them, i...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0      [21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7]   \n",
       "1                                           [22, 22]   \n",
       "2                            [22, 6, 22, 22, 23, 11]   \n",
       "3  [22, 38, 12, 21, 15, 29, 16, 22, 21, 15, 12, 1...   \n",
       "4  [10, 22, 38, 29, 21, 37, 28, 15, 12, 21, 21, 1...   \n",
       "\n",
       "                                          chunk_tags  \\\n",
       "0      [11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0]   \n",
       "1                                           [11, 12]   \n",
       "2                            [11, 0, 11, 12, 12, 12]   \n",
       "3  [11, 21, 11, 12, 13, 11, 12, 12, 12, 13, 11, 1...   \n",
       "4  [0, 11, 21, 11, 12, 21, 11, 13, 11, 12, 12, 13...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0               [0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0]  \n",
       "1                                             [1, 2]  \n",
       "2                                 [5, 0, 5, 6, 6, 0]  \n",
       "3  [5, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('conll2003')  # download a dataset\n",
    "\n",
    "# now you can preview a few examples\n",
    "dataset['test'].to_pandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "label_list = dataset[\"train\"].features[f\"pos_tags\"].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'germany', \"'\", 's', 'representative', 'to', 'the', 'european', 'union', \"'\", 's', 'veterinary', 'committee', 'werner', 'z', '##wing', '##mann', 'said', 'on', 'wednesday', 'consumers', 'should', 'buy', 'sheep', '##me', '##at', 'from', 'countries', 'other', 'than', 'britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.', '[SEP]']\n",
      "[-100, 22, 27, 27, 21, 35, 12, 22, 22, 27, 27, 16, 21, 22, 22, 22, 22, 38, 15, 22, 24, 20, 37, 21, 21, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7, -100] [101, 2762, 1005, 1055, 4387, 2000, 1996, 2647, 2586, 1005, 1055, 15651, 2837, 14121, 1062, 9328, 5804, 2056, 2006, 9317, 10390, 2323, 4965, 8351, 4168, 4017, 2013, 3032, 2060, 2084, 3725, 2127, 1996, 4045, 6040, 2001, 24509, 1012, 102]\n",
      "39 39\n"
     ]
    }
   ],
   "source": [
    "example = dataset[\"train\"][4]\n",
    "\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)\n",
    "\n",
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[f\"pos_tags\"][i] for i in word_ids]\n",
    "print(aligned_labels, tokenized_input[\"input_ids\"])\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], [101, 2848, 13934, 102], [101, 9371, 2727, 1011, 5511, 1011, 2570, 102], [101, 1996, 2647, 3222, 2056, 2006, 9432, 2009, 18335, 2007, 2446, 6040, 2000, 10390, 2000, 18454, 2078, 2329, 12559, 2127, 6529, 5646, 3251, 5506, 11190, 4295, 2064, 2022, 11860, 2000, 8351, 1012, 102], [101, 2762, 1005, 1055, 4387, 2000, 1996, 2647, 2586, 1005, 1055, 15651, 2837, 14121, 1062, 9328, 5804, 2056, 2006, 9317, 10390, 2323, 4965, 8351, 4168, 4017, 2013, 3032, 2060, 2084, 3725, 2127, 1996, 4045, 6040, 2001, 24509, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 22, 42, 16, 21, 35, 37, 16, 21, 7, -100], [-100, 22, 22, -100], [-100, 22, 11, 11, 11, 11, 11, -100], [-100, 12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7, -100], [-100, 22, 27, 27, 21, 35, 12, 22, 22, 27, 27, 16, 21, 22, 22, 22, 22, 38, 15, 22, 24, 20, 37, 21, 21, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7, -100]]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"pos_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenize_and_align_labels(dataset['train'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/emukans/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-eaf3312fba82cd57.arrow\n",
      "Loading cached processed dataset at /home/emukans/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-f9fca18b1e3311fd.arrow\n",
      "Loading cached processed dataset at /home/emukans/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-2c0c351f8b46192b.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Existing model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "batch_size = 128\n",
    "early_stopping_patience = 5\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_checkpoint}-finetuned-pos\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_328979/3097260500.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/adapter-pos-example/venv/lib/python3.8/site-packages/transformers/trainer.py:445\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device:\n\u001b[0;32m--> 445\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m~/adapter-pos-example/venv/lib/python3.8/site-packages/transformers/trainer.py:700\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 700\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/adapter-pos-example/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/adapter-pos-example/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/adapter-pos-example/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/adapter-pos-example/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/adapter-pos-example/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/adapter-pos-example/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: pos_tags, ner_tags, id, chunk_tags, tokens. If pos_tags, ner_tags, id, chunk_tags, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14041\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 330\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='330' max='330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [330/330 01:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.619398</td>\n",
       "      <td>0.819528</td>\n",
       "      <td>0.827005</td>\n",
       "      <td>0.823250</td>\n",
       "      <td>0.871400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.442550</td>\n",
       "      <td>0.869425</td>\n",
       "      <td>0.871746</td>\n",
       "      <td>0.870584</td>\n",
       "      <td>0.900106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.416922</td>\n",
       "      <td>0.876825</td>\n",
       "      <td>0.882931</td>\n",
       "      <td>0.879868</td>\n",
       "      <td>0.905571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: pos_tags, ner_tags, id, chunk_tags, tokens. If pos_tags, ner_tags, id, chunk_tags, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 128\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NNP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: : seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: IN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: . seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VBD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NNS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: TO seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRP$ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: , seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: RB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: MD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: JJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: RP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VBN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VBG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VBZ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VBP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: POS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: WP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: WDT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ( seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ) seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NNPS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: JJS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: $ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: WRB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: \" seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: EX seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: WP$ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: '' seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: RBR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: RBS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: FW seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: JJR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: UH seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PDT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NN|SYM seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: pos_tags, ner_tags, id, chunk_tags, tokens. If pos_tags, ner_tags, id, chunk_tags, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 128\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: pos_tags, ner_tags, id, chunk_tags, tokens. If pos_tags, ner_tags, id, chunk_tags, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 128\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=330, training_loss=0.8422676780007102, metrics={'train_runtime': 88.5726, 'train_samples_per_second': 475.576, 'train_steps_per_second': 3.726, 'total_flos': 671518763395206.0, 'train_loss': 0.8422676780007102, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: pos_tags, ner_tags, id, chunk_tags, tokens. If pos_tags, ner_tags, id, chunk_tags, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.41692161560058594,\n",
       " 'eval_precision': 0.8768252132427353,\n",
       " 'eval_recall': 0.8829309717335921,\n",
       " 'eval_f1': 0.8798675000302235,\n",
       " 'eval_accuracy': 0.9055713536784914,\n",
       " 'eval_runtime': 4.2722,\n",
       " 'eval_samples_per_second': 760.731,\n",
       " 'eval_steps_per_second': 6.086,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: pos_tags, ner_tags, id, chunk_tags, tokens. If pos_tags, ner_tags, id, chunk_tags, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\"'\": {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 11},\n",
       " 'B': {'precision': 0.7939240506329114,\n",
       "  'recall': 0.8222338751966439,\n",
       "  'f1': 0.8078310149407523,\n",
       "  'number': 1907},\n",
       " 'BD': {'precision': 0.926283457656867,\n",
       "  'recall': 0.9491906474820144,\n",
       "  'f1': 0.9375971574505886,\n",
       "  'number': 2224},\n",
       " 'BG': {'precision': 0.8772727272727273,\n",
       "  'recall': 0.8283261802575107,\n",
       "  'f1': 0.8520971302428255,\n",
       "  'number': 699},\n",
       " 'BN': {'precision': 0.8486238532110092,\n",
       "  'recall': 0.7974137931034483,\n",
       "  'f1': 0.8222222222222222,\n",
       "  'number': 928},\n",
       " 'BP': {'precision': 0.8672566371681416,\n",
       "  'recall': 0.8054794520547945,\n",
       "  'f1': 0.8352272727272728,\n",
       "  'number': 365},\n",
       " 'BR': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 53},\n",
       " 'BS': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 18},\n",
       " 'BZ': {'precision': 0.9166666666666666,\n",
       "  'recall': 0.9076620825147348,\n",
       "  'f1': 0.912142152023692,\n",
       "  'number': 509},\n",
       " 'C': {'precision': 0.9967845659163987,\n",
       "  'recall': 0.9978540772532188,\n",
       "  'f1': 0.9973190348525468,\n",
       "  'number': 932},\n",
       " 'D': {'precision': 0.8823529411764706,\n",
       "  'recall': 0.9563602599814299,\n",
       "  'f1': 0.917867221149562,\n",
       "  'number': 3231},\n",
       " 'DT': {'precision': 0.9583333333333334,\n",
       "  'recall': 0.8518518518518519,\n",
       "  'f1': 0.9019607843137256,\n",
       "  'number': 162},\n",
       " 'H': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 5},\n",
       " 'J': {'precision': 0.7309982486865149,\n",
       "  'recall': 0.7547920433996383,\n",
       "  'f1': 0.7427046263345196,\n",
       "  'number': 2765},\n",
       " 'JR': {'precision': 0.7058823529411765,\n",
       "  'recall': 0.11428571428571428,\n",
       "  'f1': 0.19672131147540983,\n",
       "  'number': 105},\n",
       " 'JS': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 78},\n",
       " 'N': {'precision': 0.8522823586390604,\n",
       "  'recall': 0.8639990180434516,\n",
       "  'f1': 0.8581006948677313,\n",
       "  'number': 8147},\n",
       " 'NP': {'precision': 0.778805582603051,\n",
       "  'recall': 0.8245704467353951,\n",
       "  'f1': 0.801034885661826,\n",
       "  'number': 5820},\n",
       " 'NPS': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 165},\n",
       " 'NS': {'precision': 0.8861291555215896,\n",
       "  'recall': 0.9328238133547868,\n",
       "  'f1': 0.9088771310993533,\n",
       "  'number': 2486},\n",
       " 'N|SYM': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'O': {'precision': 0.9977802441731409,\n",
       "  'recall': 0.9933701657458563,\n",
       "  'f1': 0.9955703211517165,\n",
       "  'number': 905},\n",
       " 'OS': {'precision': 0.9789719626168224,\n",
       "  'recall': 0.9905437352245863,\n",
       "  'f1': 0.9847238542890717,\n",
       "  'number': 423},\n",
       " 'P': {'precision': 0.900497512437811,\n",
       "  'recall': 0.6581818181818182,\n",
       "  'f1': 0.7605042016806723,\n",
       "  'number': 275},\n",
       " 'P$': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 9},\n",
       " 'RB': {'precision': 1.0,\n",
       "  'recall': 0.5591397849462365,\n",
       "  'f1': 0.7172413793103448,\n",
       "  'number': 93},\n",
       " 'RP': {'precision': 0.9929906542056075,\n",
       "  'recall': 0.9860788863109049,\n",
       "  'f1': 0.989522700814901,\n",
       "  'number': 862},\n",
       " 'RP$': {'precision': 0.9882629107981221,\n",
       "  'recall': 0.9976303317535545,\n",
       "  'f1': 0.9929245283018868,\n",
       "  'number': 422},\n",
       " 'S': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'T': {'precision': 0.9816228442182641,\n",
       "  'recall': 0.9888920535459983,\n",
       "  'f1': 0.985244040862656,\n",
       "  'number': 3511},\n",
       " 'W': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 24},\n",
       " 'X': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 40},\n",
       " 'YM': {'precision': 0.7297297297297297,\n",
       "  'recall': 0.313953488372093,\n",
       "  'f1': 0.43902439024390244,\n",
       "  'number': 86},\n",
       " '_': {'precision': 0.9885960466294982,\n",
       "  'recall': 0.9868454338477106,\n",
       "  'f1': 0.9877199645524751,\n",
       "  'number': 3953},\n",
       " 'overall_precision': 0.8768252132427353,\n",
       " 'overall_recall': 0.8829309717335921,\n",
       " 'overall_f1': 0.8798675000302235,\n",
       " 'overall_accuracy': 0.9055713536784914}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training own adapter layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoAdapterModel, AdapterConfig\n",
    "\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "\n",
    "adapter_config = AdapterConfig.load(\"pfeiffer\")\n",
    "\n",
    "model.add_adapter(\"pos\", config=adapter_config)\n",
    "model.set_active_adapters(\"pos\")\n",
    "model.train_adapter(\"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, id, ner_tags, tokens, pos_tags. If chunk_tags, id, ner_tags, tokens, pos_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14041\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 330\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='330' max='330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [330/330 01:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.280522</td>\n",
       "      <td>0.061359</td>\n",
       "      <td>0.038069</td>\n",
       "      <td>0.046986</td>\n",
       "      <td>0.209351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.678600</td>\n",
       "      <td>0.079088</td>\n",
       "      <td>0.025767</td>\n",
       "      <td>0.038870</td>\n",
       "      <td>0.246858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.558509</td>\n",
       "      <td>0.119166</td>\n",
       "      <td>0.039646</td>\n",
       "      <td>0.059497</td>\n",
       "      <td>0.268734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, id, ner_tags, tokens, pos_tags. If chunk_tags, id, ner_tags, tokens, pos_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 128\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NNP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: : seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: IN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: . seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VBD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NNS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: TO seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRP$ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: , seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: RB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: MD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: JJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: RP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VBN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VBG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VBZ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VBP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: POS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: WP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: WDT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ( seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ) seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NNPS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: JJS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: $ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: WRB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: \" seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: EX seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: WP$ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: '' seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: RBR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: RBS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: FW seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: JJR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: UH seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PDT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NN|SYM seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/emukans/adapter-pos-example/venv/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, id, ner_tags, tokens, pos_tags. If chunk_tags, id, ner_tags, tokens, pos_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 128\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, id, ner_tags, tokens, pos_tags. If chunk_tags, id, ner_tags, tokens, pos_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 128\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=330, training_loss=3.0687884706439394, metrics={'train_runtime': 63.9031, 'train_samples_per_second': 659.17, 'train_steps_per_second': 5.164, 'total_flos': 678574955012550.0, 'train_loss': 3.0687884706439394, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, id, ner_tags, tokens, pos_tags. If chunk_tags, id, ner_tags, tokens, pos_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.558509111404419,\n",
       " 'eval_precision': 0.1191656942823804,\n",
       " 'eval_recall': 0.03964576003882082,\n",
       " 'eval_f1': 0.059497150763740965,\n",
       " 'eval_accuracy': 0.2687340143295153,\n",
       " 'eval_runtime': 4.5533,\n",
       " 'eval_samples_per_second': 713.775,\n",
       " 'eval_steps_per_second': 5.71,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, id, ner_tags, tokens, pos_tags. If chunk_tags, id, ner_tags, tokens, pos_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3453\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.3747849464416504,\n",
       " 'eval_precision': 0.1593105437761431,\n",
       " 'eval_recall': 0.06172115115954177,\n",
       " 'eval_f1': 0.08897212824230707,\n",
       " 'eval_accuracy': 0.34509011808576756,\n",
       " 'eval_runtime': 4.3058,\n",
       " 'eval_samples_per_second': 801.934,\n",
       " 'eval_steps_per_second': 6.271,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, id, ner_tags, tokens, pos_tags. If chunk_tags, id, ner_tags, tokens, pos_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3453\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\"'\": {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 14},\n",
       " 'B': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1625},\n",
       " 'BD': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1687},\n",
       " 'BG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 480},\n",
       " 'BN': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 816},\n",
       " 'BP': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 331},\n",
       " 'BR': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 43},\n",
       " 'BS': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 9},\n",
       " 'BZ': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 501},\n",
       " 'C': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 765},\n",
       " 'D': {'precision': 0.171667965705378,\n",
       "  'recall': 0.25558456628952714,\n",
       "  'f1': 0.2053852430353188,\n",
       "  'number': 3447},\n",
       " 'DT': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 115},\n",
       " 'H': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 7},\n",
       " 'J': {'precision': 0.020833333333333332,\n",
       "  'recall': 0.00045167118337850043,\n",
       "  'f1': 0.0008841732979664013,\n",
       "  'number': 2214},\n",
       " 'JR': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 92},\n",
       " 'JS': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 56},\n",
       " 'N': {'precision': 0.12082262210796915,\n",
       "  'recall': 0.02106993424985057,\n",
       "  'f1': 0.03588242778979514,\n",
       "  'number': 6692},\n",
       " 'NP': {'precision': 0.15683663959526029,\n",
       "  'recall': 0.20626860444755735,\n",
       "  'f1': 0.1781878687036757,\n",
       "  'number': 5711},\n",
       " 'NPS': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 160},\n",
       " 'NS': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2157},\n",
       " 'O': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 818},\n",
       " 'OS': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 347},\n",
       " 'P': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 220},\n",
       " 'P$': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 9},\n",
       " 'RB': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 74},\n",
       " 'RP': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 604},\n",
       " 'RP$': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 296},\n",
       " 'S': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 23},\n",
       " 'T': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2791},\n",
       " 'W': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 32},\n",
       " 'X': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 34},\n",
       " 'YM': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 118},\n",
       " '_': {'precision': 1.0,\n",
       "  'recall': 0.002284408909194746,\n",
       "  'f1': 0.004558404558404559,\n",
       "  'number': 3502},\n",
       " 'overall_precision': 0.1593105437761431,\n",
       " 'overall_recall': 0.06172115115954177,\n",
       " 'overall_f1': 0.08897212824230707,\n",
       " 'overall_accuracy': 0.34509011808576756}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
